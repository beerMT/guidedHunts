{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threat Hunting in Apache Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "The following is interactive Jupyter notebook code that parsed Apache access logs and ingested them into Pandas data frames for statistical analysis. The outputs from executed notebook cells will be saved to CSV and PNG files respectively and can be found in the images_folder or output_folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directions\n",
    "\n",
    "This project will be initialized with the following directory schema:\n",
    "    \n",
    "```\n",
    "project_name_folder (user creates this)\n",
    "|  \n",
    "+-- apache_accessLogs_hunt.ipynb (user would start by placing this notebook into an empty project folder) \n",
    "|  \n",
    "+-- project_README.md (user would document external project dependencies, etc.) \n",
    "|\n",
    "+-- data_logs_folder (this gets created, user can then upload/move log files to this dir)\n",
    "|    |\n",
    "|    +-- original_log.csv/.log/.gz etc.\n",
    "|    |\n",
    "|    +-- output_folder (this gets created, used to store files that are outputs of Pandas DF manipulation)\n",
    "|    |\n",
    "|    \\-- images_folder (this gets created, used to store image files that are saved graphs/visualizations)\n",
    "|\n",
    "+-- max_mind_db (this gets created, used to store GeoIP database)\n",
    "     | \n",
    "     +-- GeoLite2-ASN.mmdb\n",
    "     |\n",
    "     \\-- GeoLite2-City.mmdb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange the workbench and tools required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to make sure conda env path is appropriate for Jupy NB context (risk_assessment)\n",
    "#Sys.executable needs to be something like: /Users/bpalm/opt/anaconda3/envs/risk_assessment/bin/python\n",
    "import sys\n",
    "\n",
    "#Then do check to see if risk_assessment exists\n",
    "if 'threat_hunt' not in sys.executable:\n",
    "    print('Error: Conda Env is not set to the correct path! Trouble shoot conda env path.')\n",
    "\n",
    "#Check python version\n",
    "MIN_REQ_PYTHON = (3,6)\n",
    "if sys.version_info < MIN_REQ_PYTHON:\n",
    "    print('Check the Kernel->Change Kernel menu and ensure that Python 3.6')\n",
    "    print('or later is selected as the active kernel.')\n",
    "    sys.exit(\"Python %s.%s or later is required.\\n\" % MIN_REQ_PYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File upload section where the directory structure gets built and then analyst could upload their file(s) to be worked on\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "#Check for .env files and msticpyconfig.yaml\n",
    "dot_env = Path.cwd() / '.env'\n",
    "if not dot_env.is_file():\n",
    "    print('Please create a .env file in the current working directory. This file contains API keys for HIBP, Shodan, etc pivots.')\n",
    "\n",
    "mstic_config = Path.cwd() / 'msticpyconfig.yaml'\n",
    "if not mstic_config.is_file():\n",
    "    print('Please create a msticpyconfig.yaml file in the current working directory.')\n",
    "\n",
    "#Print Markdown in output\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "#Make data_logs_folder\n",
    "data_logs_folder = Path.cwd() / 'data_logs_folder'\n",
    "if Path(data_logs_folder).exists() != True:\n",
    "    data_logs_folder.mkdir()\n",
    "    printmd('**data_logs_folder created**')\n",
    "\n",
    "else:\n",
    "    printmd('**data_logs_folder already present**')\n",
    "\n",
    "#Make images_folder\n",
    "images_folder = Path.cwd() / 'images_folder'\n",
    "if Path(images_folder).exists() != True:\n",
    "    images_folder.mkdir()\n",
    "    printmd('**images_folder created**')\n",
    "\n",
    "else:\n",
    "    printmd('**images_folder already present**')\n",
    "\n",
    "#Make output_folder\n",
    "output_folder = Path.cwd() / 'output_folder'\n",
    "if Path(output_folder).exists() != True:\n",
    "    output_folder.mkdir()\n",
    "    printmd('**output_folder created**')\n",
    "\n",
    "else:\n",
    "    printmd('**output_folder already present**')\n",
    "\n",
    "#Make max_mind_db_folder - this will get created with the msticpy bootstrapping\n",
    "#This module import addresses the MSTICpy / MaxMind db bootstrap chore\n",
    "import msticpy.sectools as sectools\n",
    "from msticpy.nbtools import *\n",
    "from msticpy.nbtools.entityschema import IpAddress, GeoLocation\n",
    "from msticpy.sectools.geoip import GeoLiteLookup, IPStackLookup\n",
    "\n",
    "#Quick check on dir path and file existing\n",
    "max_mind_path = Path.cwd()/'max_mind_db'\n",
    "max_mind_db = Path(max_mind_path/'GeoLite2-City.mmdb')\n",
    "\n",
    "if Path(max_mind_path).exists() == True:\n",
    "    if max_mind_db.is_file():\n",
    "        printmd('**max_mind_db dir and mmdb already present**')\n",
    "\n",
    "#This initiates the maxmind db pull and dir creation or the db update process\n",
    "iplocation = GeoLiteLookup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import geoip2.database\n",
    "import gzip\n",
    "from ipyfilechooser import FileChooser\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, Box, Dropdown, RadioButtons, Output\n",
    "from ipwhois import IPWhois\n",
    "import json\n",
    "import kaleido\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import pycountry\n",
    "import requests\n",
    "from user_agents import parse\n",
    "import whois\n",
    "\n",
    "#Doesn't truncate Pandas DF if you uncomment below\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "#This function allows pio.renderers.default = w.value to be set during an on_change event and not in the following NB cell\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        pio.renderers.default = selecta.value\n",
    "\n",
    "def check_file(change):\n",
    "    if fc.selected_filename == None:\n",
    "        #raise alert\n",
    "        caption.value = 'File needs to be selected in FileChooser drop down.'\n",
    "\n",
    "    elif change.new not in fc.selected_filename:\n",
    "        #raise alert\n",
    "        caption.value = 'File type does not match the file selected.'\n",
    "    \n",
    "    elif change.new in fc.selected_filename:\n",
    "        caption.value = 'File type matches file selected, please continue!'\n",
    "\n",
    "#Use drop down widget to establish path to directory that holds the log files\n",
    "# Create and display a FileChooser widget\n",
    "fc = FileChooser()\n",
    "fc.use_dir_icons = True\n",
    "fc.default_path = str(Path.cwd()/'data_logs_folder')\n",
    "\n",
    "caption = widgets.Label(value='The log type has not been selected...')\n",
    "\n",
    "#Use radio button widget to establish file type\n",
    "selecta_ext = RadioButtons(\n",
    "    options=['.csv','.txt','.json','.log'],\n",
    "    value='.csv', # Defaults to '.csv'\n",
    "    style={'description_width':'initial'},\n",
    "    layout={'width': 'max-content'}, # If the items' names are long\n",
    "    description='Select log file type to be parsed:',\n",
    "    disabled=False\n",
    ")\n",
    "selecta_ext.observe(check_file,names='value')\n",
    "\n",
    "#Fix for plotly graph rendering in Firefox browser - notebooks, labs, and PDF export\n",
    "selecta = Dropdown(\n",
    "    options=['notebook', 'plotly_mimetype', 'notebook+plotly_mimetype+pdf'],\n",
    "    value='notebook',\n",
    "    description='Plotly pio.renderers setting:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "selecta.observe(on_change)\n",
    "\n",
    "#Brad TODO: make these instructions for selections in markdown!\n",
    "print(\"Please navigate to the directory that contains the logs to be analyzed.\")\n",
    "print(\"Select a log file in the directory with the File Chooser.\")\n",
    "print(\"Select the file type of the logs to be imported.\")\n",
    "print(\"The selected path and file type will be stored and used to analyze all file extensions of that type in the directory.\")\n",
    "display(fc, caption, selecta_ext, selecta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Materials onto the workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logs were provided in advance and were downloaded as a zip file. The files were unzipped and placed into the data_logs_folder directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing for Apache logs\n",
    "#The below did not work and got hung up on a few logs' timezones\n",
    "\n",
    "\n",
    "\n",
    "# from datetime import datetime\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from pandas import DataFrame, Series\n",
    "# import pytz\n",
    "# import re\n",
    "\n",
    "# def parse_str(x):\n",
    "#     \"\"\"\n",
    "#     Returns the string delimited by two characters.\n",
    "\n",
    "#     Example:\n",
    "#         `>>> parse_str('[my string]')`\n",
    "#         `'my string'`\n",
    "#     \"\"\"\n",
    "#     return x[1:-1]\n",
    "\n",
    "# def parse_datetime(x):\n",
    "#     '''\n",
    "#     Parses datetime with timezone formatted as:\n",
    "#         `[day/month/year:hour:minute:second zone]`\n",
    "\n",
    "#     Example:\n",
    "#         `>>> parse_datetime('13/Nov/2015:11:45:42 +0000')`\n",
    "#         `datetime.datetime(2015, 11, 3, 11, 45, 4, tzinfo=<UTC>)`\n",
    "\n",
    "#     Due to problems parsing the timezone (`%z`) with `datetime.strptime`, the\n",
    "#     timezone will be obtained using the `pytz` library.\n",
    "#     '''\n",
    "#     dt = datetime.strptime(x[1:-7], '%d/%b/%Y:%H:%M:%S')\n",
    "#     dt_tz = int(x[-6:-3])*60+int(x[-3:-1])\n",
    "#     return dt.replace(tzinfo=pytz.FixedOffset(dt_tz))\n",
    "\n",
    "\n",
    "# def parse_datetime(x):\n",
    "#     dt = datetime.strptime(x[1:-7], '%d/%b/%Y:%H:%M:%S') %m/%d/%Y %H:%M:%S\n",
    "#     dt_tz = int(x[-6:-3])*60+int(x[-3:-1])\n",
    "#     return dt.replace(tzinfo=pytz.FixedOffset(dt_tz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# apache_log_path = Path.cwd()/'data_logs_folder/help1.20211209-1211/local1/apache2/logs/help/access_2021_11_12_21_49_52.log'\n",
    "\n",
    "# data = pd.read_csv(\n",
    "#     apache_log_path,\n",
    "#     sep=r'\\s(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)(?![^\\[]*\\])',\n",
    "#     engine='python',\n",
    "#     na_values='-',\n",
    "#     header=None,\n",
    "#     usecols=[0, 3, 4, 5, 6, 7, 8],\n",
    "#     names=['ip', 'time', 'request', 'status', 'size', 'referer', 'user_agent'],\n",
    "#     converters={'time': parse_datetime,\n",
    "#                 'request': parse_str,\n",
    "#                 'status': int,\n",
    "#                 'size': int,\n",
    "#                 'referer': parse_str,\n",
    "#                 'user_agent': parse_str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseApacheLogs(filename):\n",
    "    fields = ['host', 'identity', 'user', 'time_part1', 'time_part2', 'cmd_path_proto',\n",
    "             'http_code', 'response_bytes', 'referer', 'user_agent']\n",
    "    \n",
    "    data = pd.read_csv(filename, sep=' ', header=None, names=fields, na_values=['-'], dtype={\"cmd_path_proto\": \"string\", \"http_code\": \"string\", \"user_agent\": \"string\"})\n",
    "\n",
    "    # Panda's parser mistakenly splits the date into two columns, so we must concatenate them\n",
    "    time = data.time_part1 + data.time_part2\n",
    "    time_trimmed = time.map(lambda s: s.strip('[]').split('-')[0]) # Drop the timezone for simplicity\n",
    "    data['time'] = pd.to_datetime(time_trimmed, format='%d/%b/%Y:%H:%M:%S')\n",
    "    \n",
    "    # # Split column `cmd_path_proto` into three columns, and decode the URL (ex: '%20' => ' ')\n",
    "    # data['command'], data['path'], data['protocol'] = zip(*data['cmd_path_proto'].str.split().tolist())\n",
    "    # data['path'] = data['path'].map(lambda s: unquote(s))\n",
    "    request = data.pop('cmd_path_proto').str.split()\n",
    "    data['method'] = request.str[0]\n",
    "    data['resource'] = request.str[1]\n",
    "    data['protocol'] = request.str[2]\n",
    "    data=data[['time','time_part1', 'time_part2','host','identity', 'user','method','resource','protocol','http_code','response_bytes','referer','user_agent']]\n",
    "    \n",
    "    # # Drop the fixed columns and any empty ones\n",
    "    data1 = data.drop(['time_part1', 'time_part2'], axis=1)\n",
    "    return data1.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now what paths do we need to move forward\n",
    "# - data_logs_folder holds all the log files and can have subdirectories with dates\n",
    "# - images_folder is a subdir of data_logs_folder\n",
    "# - output_folder is a subdir of data_logs_folder\n",
    "# - max_mind_db is at same level/hierarchy as data_logs_folder\n",
    "\n",
    "#Working folder will be the selected_path from File Chooser fc var\n",
    "working_folder = Path(fc.selected_path)\n",
    "\n",
    "#Establish array of log files to be processed using Pathlib methods iterdir and is_file\n",
    "# Had to wrap the iterdir with sorted since it wasn't loading them by time stamp!\n",
    "# Make sure there are no hidden files or non-log file types,\n",
    "# l only gets added if is_file and endswith .log and has access in filename\n",
    "file_list = [l for l in sorted(working_folder.iterdir()) if l.is_file() and (l.name.endswith(selecta_ext.value) and \"access\" in l.name)]\n",
    "assert (len(file_list) > 0), \"No log files were found in dir!\"\n",
    "\n",
    "# crate a list to add dataframes to\n",
    "apache_list = list()\n",
    "\n",
    "completedFiles = 0\n",
    "eventsProcessed = 0\n",
    "print(\"Number of logs to be analyzed: \",len(file_list))\n",
    "\n",
    "for log in file_list:\n",
    "    try:\n",
    "        temp = parseApacheLogs(log)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print(f\"The error occurred in this log file: {log}\")\n",
    "    \n",
    "    eventsProcessed += len(temp)\n",
    "\n",
    "    # normalize the file and append it to the list of dataframe \n",
    "    apache_list.append(temp)\n",
    "    \n",
    "    #Print out finished analysis of log name\n",
    "    completedFiles += 1\n",
    "\n",
    "# concat the files into a single dataframe\n",
    "apache = pd.concat(apache_list).reset_index(drop=True)\n",
    "\n",
    "print(\"Number of logs analyzed:\",completedFiles) \n",
    "print(\"Number of events processed:\", eventsProcessed)\n",
    "\n",
    "#Now we have apache df loaded onto the workbench, time to hunt!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO:* Better option for parsing logs moving forward, as it handles the time format issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from apachelogs import LogParser\n",
    "# parser = LogParser(\"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\"\")\n",
    "\n",
    "# apache_log_path = Path.cwd()/'data_logs_folder/help1.20211209-1211/local1/apache2/logs/help/access_2021_11_12_21_49_52.log'\n",
    "\n",
    "# with open(apache_log_path) as fp:\n",
    "#     for entry in parser.parse_lines(fp):\n",
    "#         print(str(entry.request_time), entry.request_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP! - Determine if the right materials have been loaded on the workbench - do timestamps seem correct, do number of rows/events seem correct?\n",
    "There was an issue with loading multiple CSV files (read_csv) into Pandas DF that lead me to question if read_json behaves in the same way.\n",
    "\n",
    "* [X] dataframe that was loaded checks out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(apache.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(apache.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_key_str = 'HTTP/1.1\"'\n",
    "h1_boolean_findings = apache['http_code'].str.contains(h1_key_str)\n",
    "total_occurence = h1_boolean_findings.sum()\n",
    "if(total_occurence > 0):\n",
    "    print(f\"Yes, {h1_key_str} is present in the data frame\")\n",
    "elif(total_occurence == 0):\n",
    "    print(f\"No, {h1_key_str} is not present in the data frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the rows that contain key_str for further inspection\n",
    "showString_df=apache[apache.http_code.isin([h1_key_str])]\n",
    "display(showString_df)\n",
    "showString_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_drop = apache[apache['http_code'] != 'HTTP/1.1\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(apache_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_drop = apache_drop.astype({\"http_code\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_drop.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "Each threat hunt starts with establishing the following criteria:\n",
    "* What visibility do I have with regard to identifying attacker behavior? Identifying app misconfigurations? Identifying the efficacy of security controls?\n",
    "* What vantage point does this data provide and what attacker behaviors can be identified within it?\n",
    "\n",
    "Once the visibility is established, a hunt is crafted that takes into account the data available and what techniques can be used to identify attacker behavior. A hunt is centered around a well formed hypothesis that seeks to address a single question about the data and attacker.\n",
    "\n",
    "Apache access logs provide insight into:\n",
    "* Time\n",
    "* Host (client IP)\n",
    "* Request (method, resource, protocol)\n",
    "* HTTP Code (successful, unsuccessful, etc)\n",
    "* Referer (redirect, link)\n",
    "* User-Agent\n",
    "\n",
    "The following hunts addressed different areas of the Attacker's Kill Chain and could be explored within the data set, given the visibility provided by the CloudTrail logs: \n",
    "\n",
    "* Persistance (Hunt 1 - H1)\n",
    "    - [X] Webshell - POST requests that are valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Stack Count of HTTP Codes, Client IP Addresses, Requests, Referer User-Agents \n",
    "\n",
    "See what falls out of top talkers/ top events prime the analyst pump!\n",
    "\n",
    "Column headers:\n",
    "\n",
    "`['time','host','method','resource','protocol','http_code','response_bytes','referer','user_agent']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTTP Method Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_drop['method'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Client IP Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_drop['host'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our tail looks pretty long....lets see\n",
    "aggregateIPs = pd.DataFrame(apache_drop.host.value_counts().reset_index().values, columns=[\"clientIP\", \"count\"])\n",
    "ip_head_var = 10\n",
    "print('The top',ip_head_var,'IP talkers from stack counting are shown below:')\n",
    "display(aggregateIPs.head(ip_head_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotly graphing\n",
    "fig = px.line(aggregateIPs, x='clientIP', y='count', title='Long Tail Analysis - Most to Least Frequently Occurring IPs')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTTP Status Code Stacking\n",
    "\n",
    "* 302 - Redirect\n",
    "* 404 - Not Found, can drop these\n",
    "* 403 - Forbidden, can drop these or look at these specifically\n",
    "* 400 - Bad Request, can drop these\n",
    "* 206 - Partial Content Success\n",
    "* 304 - Not Modified client redirect, can drop these\n",
    "* 405 - Method Not Allowed, can drop these\n",
    "* 401 - Unauthorized, can drop these\n",
    "* 500 - Internal Server Error, can drop these\n",
    "* 502 - Bad Gateway, can drop these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_drop['http_code'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence (H1)\n",
    "\n",
    "**Hypothesis #1:** If there is an attacker actively exploiting a web shell on the Confluence server, they would be interacting with a web shell that could be identified in Apache Access log data. \n",
    "\n",
    "**MITRE ATT&ACK**\n",
    "- Tactic: Persistence\n",
    "- Technique/Sub-Technique: T1505.003\n",
    "\n",
    "**Assumption:** Attackers need to maintain persistent access to target for C2 and additional exploitation. Web shells could be the primary point of origin (patient 0) or it could be secondary or fallback path for persistence.\n",
    "\n",
    "**Question:** Do the Apache access logs contain evidence of a remote host interacting with a web shell on the Confluence server?\n",
    "\n",
    "**Method:** An attacker generating a successful request to a web shell will use an HTTP POST method with a lower frequency requested resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status Code + HTTP Method Filter and then Requested Resource Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now count event + userName that you are interested in\n",
    "apache_success = apache_drop.copy()\n",
    "apache_success = apache_success[(apache_success['http_code'] == 200) & (apache_success['method'] == 'POST')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_success.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_success.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_success.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(apache_success))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requested Resource Stacking\n",
    "Tried this early in the objective section to get a view of the data but it was too messy and there were too many unique URI strings. This one is more manageable as the dataframe has been filtered down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_success['resource'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Pivot on /rest/tinymce/1/macro/preview String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_key_str = '/rest/tinymce/1/macro/preview'\n",
    "h2_boolean_findings = apache_success['resource'].str.contains(h2_key_str)\n",
    "total_occurence = h2_boolean_findings.sum()\n",
    "if(total_occurence > 0):\n",
    "    print(f\"Yes, {h2_key_str} is present in the data frame\")\n",
    "elif(total_occurence == 0):\n",
    "    print(f\"No, {h2_key_str} is not present in the data frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the rows that contain key_str for further inspection\n",
    "showString_df=apache_success[apache_success.resource.isin([h2_key_str])]\n",
    "display(showString_df)\n",
    "showString_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writeout to CSV file\n",
    "with open (output_folder/\"post_success_lfo.csv\", \"w\") as outfile:\n",
    "    showString_df.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Pivot on Login String\n",
    "apache_success is too narrowed for this search, so be sure to use a dataframe prior to the POST + 200 filtering..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_key_str = 'login'\n",
    "h3_boolean_findings = apache_drop['resource'].str.contains(h3_key_str)\n",
    "total_occurence = h3_boolean_findings.sum()\n",
    "if(total_occurence > 0):\n",
    "    print(f\"Yes, {h3_key_str} is present in the data frame\")\n",
    "elif(total_occurence == 0):\n",
    "    print(f\"No, {h3_key_str} is not present in the data frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_df = apache_drop[(apache_drop['resource'].str.contains(h3_key_str,na=False)) & (apache_drop['http_code'] == 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_df['resource'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeoIP Enrichment Pivot\n",
    "This is more of an ad-hoc Data-Based Hunt (DBH). This would be more helpful if you had a specific target market and anomalies could stand out based on that. However, still an interesting visualization to help build a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoip_enrich_IPs = apache_drop.copy()\n",
    "geoip_enrich_IPs = geoip_enrich_IPs['host'].value_counts().rename_axis('source_IP').reset_index(name='counts')\n",
    "geoip_enrich_IPs.size\n",
    "# Drop URLs and AWS Internal tags from source_IPs\n",
    "geoip_enrich_IPs_dropped = pd.DataFrame(geoip_enrich_IPs[geoip_enrich_IPs['source_IP'].str.contains(\"10.*\")==False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoip_enrich_IPs_dropped.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoip_enrich_IPs = apache_drop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoip_enrich_IPs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoip_enrich_IPs = apache_drop.copy()\n",
    "\n",
    "# Drop internal IPs from host column\n",
    "geoip_enrich_IPs_dropped = pd.DataFrame(geoip_enrich_IPs[geoip_enrich_IPs['host'].str.contains(\"10.*|::1|192.168*\")==False])\n",
    "# Drop AWS Health Check-User agent IPs\n",
    "geoip_enrich_IPs_dropped = geoip_enrich_IPs_dropped[~(geoip_enrich_IPs_dropped['user_agent'] == 'Amazon-Route53-Health-Check-Service (ref afa8566f-3fc3-4314-9bad-cfe2fb9dba91; report http://amzn.to/1vsZADi)')] \n",
    "#Stack count user-agents now that items have been dropped\n",
    "geoip_enrich_IPs_stacked = geoip_enrich_IPs_dropped['host'].value_counts().rename_axis('source_IP').reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoip_enrich_IPs_stacked.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(geoip_enrich_IPs_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists to hold the new series data\n",
    "alpha2 = []\n",
    "alpha3 = []\n",
    "country_name = []\n",
    "lat = []\n",
    "lon = []\n",
    "        \n",
    "reader = geoip2.database.Reader(max_mind_db)\n",
    "\n",
    "#Was recommended not to do an apply in this multi-column scenario...go figure!\n",
    "for ip in geoip_enrich_IPs_stacked['source_IP']:\n",
    "    response = reader.city(ip)\n",
    "    response_country_checked = ''\n",
    "    \n",
    "    if response.country.iso_code == None:\n",
    "        alpha2.append(response.registered_country.iso_code)\n",
    "        response_country_checked = response.registered_country.iso_code\n",
    "        \n",
    "    elif response.country.iso_code != None:\n",
    "        alpha2.append(response.country.iso_code)\n",
    "        response_country_checked = response.country.iso_code\n",
    "\n",
    "    alpha3.append(pycountry.countries.get(alpha_2=response_country_checked).alpha_3)\n",
    "    country_name.append(response.country.name)\n",
    "    lat.append(response.location.latitude)\n",
    "    lon.append(response.location.longitude)\n",
    "    \n",
    "\n",
    "geoip_enrich_IPs_stacked['alpha2_code'] = alpha2    \n",
    "geoip_enrich_IPs_stacked['alpha3_code'] = alpha3\n",
    "geoip_enrich_IPs_stacked['country_name'] = country_name\n",
    "geoip_enrich_IPs_stacked['lat'] = lat\n",
    "geoip_enrich_IPs_stacked['lon'] = lon\n",
    "\n",
    "display(geoip_enrich_IPs_stacked.head(20))\n",
    "\n",
    "#Creates new DF with sum of clientIPs per Alpha_3 ISO code, this is the format for scatter_geo map\n",
    "aggregateCountry = pd.DataFrame(geoip_enrich_IPs_stacked.drop(['country_name','alpha2_code','lat','lon'], axis=1))\n",
    "aggregateCountry = aggregateCountry.groupby(['alpha3_code','source_IP']).sum().groupby('alpha3_code').sum()\n",
    "aggregateCountry = aggregateCountry.stack().reset_index().drop('level_1', axis=1) #THIS gets the job done, but not neat....                               \n",
    "aggregateCountry.rename(columns={0:'sum'}, inplace=True)\n",
    "\n",
    "#Do ratios of US vs. non-US\n",
    "#Visualize traffic by country breakdown with pie chart\n",
    "fig2 = px.pie(aggregateCountry, values='sum', names='alpha3_code', title='Percentage of Source IP Traffic Observed by Country')\n",
    "fig2.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig2.show()\n",
    "\n",
    "#Uncomment if you need to save out image!\n",
    "output_path_png = str(images_folder)+'/ip_country_pieChart.png'\n",
    "output_path_html = str(images_folder)+'/ip_country_pieChart.html'\n",
    "\n",
    "try:\n",
    "    fig2.write_image(output_path_png)\n",
    "    fig2.write_html(output_path_html)\n",
    "\n",
    "except ValueError as con:\n",
    "    print('Connection refused error to orca')\n",
    "\n",
    "else:\n",
    "    print('Image was saved out as \"ip_country_pieChart\" in both html and png format!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick long tail check\n",
    "geoip_enrich_IPs_stacked.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Agent String Enrichment Pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Agent Stack Count\n",
    "Use this to grab top-talker - Amazon Health Check - and the filter that out of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_drop['user_agent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent_df = apache_drop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_agent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent_df = user_agent_df[~(user_agent_df['user_agent'] == 'Amazon-Route53-Health-Check-Service (ref afa8566f-3fc3-4314-9bad-cfe2fb9dba91; report http://amzn.to/1vsZADi)')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_agent_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Agent Parsing Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregateUAs = pd.DataFrame(user_agent_df.user_agent.value_counts().reset_index().values, columns=[\"userAgent\", \"count\"])\n",
    "ua_head_var = 20\n",
    "print('The top',ua_head_var,'User-Agent strings from stack counting, after filtering out AWS Health Check, are shown below:')\n",
    "display(aggregateUAs.head(ua_head_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate over UA values in column for follow-on pivot/parse\n",
    "\n",
    "#Create lists to hold the new series data\n",
    "user_parsed = []\n",
    "is_mobile = []\n",
    "is_bot = []\n",
    "\n",
    "#Was recommended to not do an apply in this multi-column scenario...go figure!\n",
    "for ua in aggregateUAs['userAgent']:\n",
    "    user_agent = parse(ua)\n",
    "    user_parsed.append(str(user_agent))\n",
    "    is_mobile.append(user_agent.is_mobile)\n",
    "    is_bot.append(user_agent.is_bot)\n",
    "\n",
    "aggregateUAs['userAgent-parsed']=user_parsed\n",
    "aggregateUAs['is-mobile']=is_mobile\n",
    "aggregateUAs['is-bot']=is_bot\n",
    "\n",
    "#ua_head_var passed in from previous cell!\n",
    "print('The top',ua_head_var,'User-Agent strings from stack counting, after the AWS Health Check was filtered, are shown below:')\n",
    "display(aggregateUAs.head(ua_head_var))\n",
    "\n",
    "#Write results to file\n",
    "with open (output_folder/\"user_agent_count_enrich.csv\", \"w\") as outfile:\n",
    "    aggregateUAs.to_csv(outfile, index=False)\n",
    "\n",
    "print('DataFrame was saved out as \"user_agent_count_enrich.csv\"')\n",
    "\n",
    "#Determine ratios for mobile vs non-mobile    \n",
    "#Determine ratios for bot vs. non-bot\n",
    "aggregateUAs_copy = aggregateUAs.copy()\n",
    "trueMobile = aggregateUAs_copy[aggregateUAs_copy['is-mobile'] == True]\n",
    "trueBots = aggregateUAs_copy[aggregateUAs_copy['is-bot'] == True]\n",
    "\n",
    "def percentage(part, whole):\n",
    "    num = 100 * float(part)/float(whole)\n",
    "    return round(num,2)\n",
    "\n",
    "print(\"\\n\")\n",
    "print('Total UAs observed: ', aggregateUAs['count'].sum())\n",
    "print('UAs determined mobile: ', trueMobile['count'].sum())\n",
    "print('Mobile traffic is',percentage(trueMobile['count'].sum(),aggregateUAs['count'].sum()) ,'% of the overall traffic')\n",
    "print(\"\\n\")\n",
    "print('Total UAs observed: ', aggregateUAs['count'].sum())\n",
    "print('UAs determined bot: ', trueBots['count'].sum())\n",
    "print('Bot traffic is',percentage(trueBots['count'].sum(),aggregateUAs['count'].sum()) ,'% of the overall traffic')\n",
    "\n",
    "#Visualize UA breakdown with pie chart\n",
    "fig5 = px.pie(aggregateUAs, values='count', names='userAgent-parsed', title='Percentage of User-Agent Traffic - Post Forward Filter')\n",
    "fig5.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig5.show()\n",
    "\n",
    "output_path_png = str(images_folder)+'/ua_pieChart.png'\n",
    "output_path_html = str(images_folder)+'/ua_pieChart.html'\n",
    "\n",
    "try:\n",
    "    fig5.write_image(output_path_png)\n",
    "    fig5.write_html(output_path_html)\n",
    "\n",
    "except ValueError as con:\n",
    "    print('Connection refused error to orca')\n",
    "\n",
    "else:\n",
    "    print('Image was saved out as \"ua_pieChart\" in both html and png format!')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
